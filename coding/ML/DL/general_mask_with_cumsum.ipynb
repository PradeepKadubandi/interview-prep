{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc829249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradeepkadubandi/GH/pradeepkadubandi/interview-prep/coding/ML/DL/venv_for_all_dl/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c88e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a neat trick for creating any attention mask in a generalized fashion\n",
    "# https://github.com/huggingface/lerobot/blob/main/src/lerobot/policies/pi0/modeling_pi0.py#L91\n",
    "\n",
    "# From the above method\n",
    "\"\"\"Copied from big_vision.\n",
    "\n",
    "Tokens can attend to valid inputs tokens which have a cumulative mask_ar\n",
    "smaller or equal to theirs. This way `mask_ar` int[B, N] can be used to\n",
    "setup several types of attention, for example:\n",
    "\n",
    "    [[1 1 1 1 1 1]]: pure causal attention.\n",
    "\n",
    "    [[0 0 0 1 1 1]]: prefix-lm attention. The first 3 tokens can attend between\n",
    "        themselves and the last 3 tokens have a causal attention. The first\n",
    "        entry could also be a 1 without changing behaviour.\n",
    "\n",
    "    [[1 0 1 0 1 0 0 1 0 0]]: causal attention between 4 blocks. Tokens of a\n",
    "        block can attend all previous blocks and all tokens on the same block.\n",
    "\n",
    "Args:\n",
    "    input_mask: bool[B, N] true if its part of the input, false if padding.\n",
    "    mask_ar: int32[B, N] mask that's 1 where previous tokens cannot depend on\n",
    "    it and 0 where it shares the same attention mask as the previous token.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23cba2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_masks = torch.Tensor([\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],  # causal\n",
    "    [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],  # prefix-lm attention\n",
    "    [1, 0, 1, 0, 1, 0, 0, 1, 0, 0],  # flexible block attention - relevant for VLAs\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e22a43c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  2.,  3.,  4.,  5.],\n",
       "        [ 1.,  1.,  2.,  2.,  3.,  3.,  3.,  4.,  4.,  4.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumsum = torch.cumsum(att_masks, dim=1)\n",
    "cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9ea56a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True, False, False, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_2d_masks = cumsum[:, None, :] <= cumsum[:, :, None]\n",
    "att_2d_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "521e3060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_masks = torch.Tensor([\n",
    "    [1,1,1,0,0,0]\n",
    "])\n",
    "pad_2d_masks = pad_masks[:, None, :] * pad_masks[:, :, None]\n",
    "pad_2d_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2513ff79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.3895e+38, -3.3895e+38, -3.3895e+38, -3.3895e+38],\n",
       "        [-3.3895e+38, -3.3895e+38, -3.3895e+38, -3.3895e+38],\n",
       "        [-3.3895e+38, -3.3895e+38, -3.3895e+38, -3.3895e+38]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length, target_length = 3, 4\n",
    "dtype = torch.bfloat16\n",
    "min_dtype = torch.finfo(dtype).min\n",
    "causal_mask = torch.full(\n",
    "        (sequence_length, target_length), fill_value=min_dtype, dtype=dtype\n",
    ")\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e111cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00, -3.3895e+38, -3.3895e+38, -3.3895e+38],\n",
       "        [ 0.0000e+00,  0.0000e+00, -3.3895e+38, -3.3895e+38],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.3895e+38]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causel_mask = torch.triu(causal_mask, diagonal=1)\n",
    "causel_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705dd73a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_for_all_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
